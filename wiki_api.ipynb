{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new spider class\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"WS\"\n",
    "\n",
    "    # Insert API call\n",
    "    start_urls = [\n",
    "        \"https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect\"\n",
    "    ]\n",
    "\n",
    "    # Use xpath to extract information\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath(\"//lh\"):\n",
    "            # Identify the type of page the link comes from: '0' means it is a Wikipedia entry\n",
    "            # Only keep Wiki entries\n",
    "            if item.xpath(\"@ns\").extract_first() == \"0\":\n",
    "                yield {\"title\": item.xpath(\"@title\").extract_first()}\n",
    "        # Get info needed to continue to the next 10 entries\n",
    "        next_page = response.xpath(\"continue/@lhcontinue\").extract_first()\n",
    "\n",
    "        # Recursively call the spider to process the next ten entries, if they exist\n",
    "        if next_page is not None:\n",
    "            next_page = \"{}&lhcontinue={}\".format(self.start_urls[0], next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess(\n",
    "    {\n",
    "        \"FEED_FORMAT\": \"json\",\n",
    "        \"FEED_URI\": \"PythonLinks.json\",\n",
    "        # robots.txt file doesn't apply to API\n",
    "        \"ROBOTSTXT_OBEY\": False,\n",
    "        \"USER_AGENT\": \"ThinkfulDataScienceBootcampCrawler (thinkful.com)\",\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,\n",
    "        \"HTTPCACHE_ENABLED\": True,\n",
    "        \"LOG_ENABLED\": False,\n",
    "        # Limit our scraper to the first 100 links\n",
    "        \"CLOSESPIDER_PAGECOUNT\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print(\"First 100 links extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Catherine Deneuve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Beyond the Fringe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Cunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>List of Welsh people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Downing College, Cambridge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title\n",
       "88           Catherine Deneuve\n",
       "89           Beyond the Fringe\n",
       "90                        Cunt\n",
       "91        List of Welsh people\n",
       "92  Downing College, Cambridge"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking whether we got data\n",
    "Monty = pd.read_json(\"PythonLinks.json\", orient=\"records\")\n",
    "print(Monty.shape)\n",
    "Monty.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
